# `qitos.models.local`

- Module Group: `qitos.models`
- Source: [qitos/models/local.py](https://github.com/Qitor/qitos/blob/main/qitos/models/local.py)

## Quick Jump

- [Classes](#classes)
- [Functions](#functions)
- [Class: `LMStudioModel`](#class-lmstudiomodel)
- [Class: `OllamaGenerateModel`](#class-ollamageneratemodel)
- [Class: `OllamaModel`](#class-ollamamodel)
- [Class: `VLLMModel`](#class-vllmmodel)

## Classes

<a id="class-lmstudiomodel"></a>
???+ note "Class: `LMStudioModel(self, model: str = 'local-model', base_url: Optional[str] = None, system_prompt: Optional[str] = None, temperature: float = 0.7, max_tokens: int = 2048, timeout: int = 120)`"
    LM Studio 本地模型实现

<a id="class-ollamageneratemodel"></a>
???+ note "Class: `OllamaGenerateModel(self, model: str = 'llama3', host: Optional[str] = None, system_prompt: Optional[str] = None, temperature: float = 0.7, timeout: int = 120)`"
    Ollama Generate API 模型

<a id="class-ollamamodel"></a>
???+ note "Class: `OllamaModel(self, model: str = 'llama3', host: Optional[str] = None, system_prompt: Optional[str] = None, temperature: float = 0.7, timeout: int = 120, format: str = 'json')`"
    Ollama 本地模型实现

<a id="class-vllmmodel"></a>
???+ note "Class: `VLLMModel(self, model: str, base_url: Optional[str] = None, system_prompt: Optional[str] = None, temperature: float = 0.7, max_tokens: int = 2048, timeout: int = 120)`"
    vLLM 本地模型实现

## Functions

- _None_

## Source Index

- [qitos/models/local.py](https://github.com/Qitor/qitos/blob/main/qitos/models/local.py)