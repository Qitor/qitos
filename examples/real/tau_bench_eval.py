"""Tau-Bench evaluation with QitOS (single task + full benchmark mode)."""

from __future__ import annotations

import argparse
import json
import random
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional

from qitos import Action, AgentModule, Decision, StateSchema, Task, TaskBudget, ToolRegistry
from qitos.benchmark import TauBenchAdapter
from qitos.evaluate import EvaluationContext, EvaluationSuite
from qitos.kit.evaluate import DSLEvaluator, ModelBasedEvaluator, RuleBasedEvaluator
from qitos.kit.metric import (
    MeanStepsMetric,
    RewardAverageMetric,
    RewardPassHatMetric,
    RewardSuccessRateMetric,
    StopReasonDistributionMetric,
    is_successful_reward,
)
from qitos.kit.parser import ReActTextParser
from qitos.kit.planning import format_action
from qitos.kit.prompts import render_prompt
from qitos.metric import MetricInput, MetricRegistry
from qitos.render import ClaudeStyleHook
from qitos.benchmark.tau_port.types import Action as TauAction

from examples.common import add_common_args, build_model_from_args, make_trace_writer, setup_workspace


SYSTEM_PROMPT = """You are a Tau-Bench task-solving agent.

Domain Wiki:
{wiki}

Policy Rules:
{rules}

Available tools:
{tool_schema}

Rules:
- Exactly one action per step.
- Use function-style action syntax.
- Use respond(content=...) when you want to talk to the user.
- Be concise and policy-compliant.

Output format:
Thought: <one-line reasoning>
Action: <tool_name>(arg=value, ...)
or
Final Answer: <very short>
"""

_APPEND_LOCK = threading.Lock()


@dataclass
class TauState(StateSchema):
    current_observation: str = ""
    scratchpad: List[str] = field(default_factory=list)
    reward: float = 0.0
    done: bool = False


class TauActionTool:
    def __init__(self, name: str, description: str, parameters: Dict[str, Any], required: List[str], runner: Any):
        from qitos.core.tool import BaseTool, ToolPermission, ToolSpec

        class _Impl(BaseTool):
            def __init__(self):
                super().__init__(
                    ToolSpec(
                        name=name,
                        description=description,
                        parameters={k: v for k, v in (parameters or {}).items()},
                        required=list(required or []),
                        permissions=ToolPermission(),
                    )
                )

            def run(self, runtime_context: Optional[Dict[str, Any]] = None, **kwargs: Any) -> Dict[str, Any]:
                return runner(name, kwargs)

        self.impl = _Impl()


class TauBenchAgent(AgentModule[TauState, Dict[str, Any], Action]):
    def __init__(self, llm: Any, tau_env: Any):
        self.tau_env = tau_env
        self._initial_obs = ""
        registry = ToolRegistry()

        for item in list(getattr(tau_env, "tools_info", []) or []):
            fn = dict(item.get("function", {}) or {})
            name = str(fn.get("name", "")).strip()
            if not name:
                continue
            desc = str(fn.get("description", ""))
            params = dict((fn.get("parameters") or {}).get("properties", {}) or {})
            req = list((fn.get("parameters") or {}).get("required", []) or [])
            wrapper = TauActionTool(name, desc, params, req, self._step_tool)
            registry.register(wrapper.impl)

        # Ensure explicit respond tool exists for conversational actions.
        if "respond" not in registry.list_tools():
            wrapper = TauActionTool(
                "respond",
                "Respond to user with plain text content",
                {"content": {"type": "string", "description": "response text"}},
                ["content"],
                self._step_tool,
            )
            registry.register(wrapper.impl)

        super().__init__(tool_registry=registry, llm=llm, model_parser=ReActTextParser())

    def init_state(self, task: str, **kwargs: Any) -> TauState:
        task_index = int(kwargs.get("task_index", 0))
        reset = self.tau_env.reset(task_index=task_index)
        obs = str(getattr(reset, "observation", ""))
        self._initial_obs = obs
        return TauState(task=task, max_steps=int(kwargs.get("max_steps", 30)), current_observation=obs)

    def observe(self, state: TauState, env_view: Dict[str, Any]) -> Dict[str, Any]:
        return {
            "task": state.task,
            "observation": state.current_observation,
            "reward": state.reward,
            "done": state.done,
            "scratchpad": list(state.scratchpad[-10:]),
        }

    def build_system_prompt(self, state: TauState) -> str | None:
        wiki = str(getattr(self.tau_env, "wiki", ""))
        rules = "\n".join([f"- {r}" for r in list(getattr(self.tau_env, "rules", []) or [])])
        return render_prompt(
            SYSTEM_PROMPT,
            {
                "wiki": wiki,
                "rules": rules,
                "tool_schema": self.tool_registry.get_tool_descriptions() if self.tool_registry else "",
            },
        )

    def prepare(self, state: TauState, observation: Dict[str, Any]) -> str:
        lines = [
            f"Task: {state.task}",
            f"Step: {state.current_step}/{state.max_steps}",
            f"Current observation: {observation.get('observation', '')}",
            f"Reward so far: {observation.get('reward', 0.0)}",
        ]
        if state.scratchpad:
            lines.append("Recent trajectory:")
            lines.extend(state.scratchpad[-8:])
        return "\n".join(lines)

    def reduce(self, state: TauState, observation: Dict[str, Any], decision: Decision[Action], action_results: List[Any]) -> TauState:
        if decision.rationale:
            state.scratchpad.append(f"Thought: {decision.rationale}")
        if decision.actions:
            state.scratchpad.append(f"Action: {format_action(decision.actions[0])}")

        if action_results:
            first = action_results[0] if isinstance(action_results[0], dict) else {"observation": str(action_results[0])}
            state.scratchpad.append(f"Observation: {first.get('observation', first)}")
            state.current_observation = str(first.get("observation", state.current_observation))
            state.reward = float(first.get("reward", state.reward or 0.0))
            state.done = bool(first.get("done", False))
            if state.done:
                state.final_result = str(first.get("observation", ""))
                state.metadata["tau_reward"] = state.reward
                state.metadata["tau_info"] = first.get("info", {})

        state.scratchpad = state.scratchpad[-40:]
        return state

    def _step_tool(self, action_name: str, kwargs: Dict[str, Any]) -> Dict[str, Any]:
        response = self.tau_env.step(TauAction(name=action_name, kwargs=kwargs))
        info = response.info.model_dump() if hasattr(response.info, "model_dump") else dict(getattr(response, "info", {}) or {})
        return {
            "status": "success",
            "action": action_name,
            "kwargs": kwargs,
            "observation": str(getattr(response, "observation", "")),
            "reward": float(getattr(response, "reward", 0.0)),
            "done": bool(getattr(response, "done", False)),
            "info": info,
        }


def _append_jsonl(path: Path, row: Dict[str, Any]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with _APPEND_LOCK, path.open("a", encoding="utf-8") as f:
        f.write(json.dumps(row, ensure_ascii=False) + "\n")


def _read_done(path: Path) -> set[str]:
    if not path.exists():
        return set()
    done = set()
    for line in path.read_text(encoding="utf-8").splitlines():
        s = line.strip()
        if not s:
            continue
        try:
            obj = json.loads(s)
        except Exception:
            continue
        tid = str(obj.get("task_id", "")).strip()
        if tid:
            done.add(tid)
    return done


def _build_tau_env(args: argparse.Namespace, task_index: int | None = None) -> Any:
    from qitos.benchmark.tau_runtime import get_tau_runtime_env

    return get_tau_runtime_env(env_name=args.tau_env, task_split=args.tau_split, task_index=task_index)


def _evaluate_one(task: Task, result: Any, include_model_judge: bool = False, llm: Any = None) -> Dict[str, Any]:
    manifest = {
        "summary": {
            "stop_reason": result.state.stop_reason,
            "final_result": result.state.final_result,
            "steps": result.step_count,
        }
    }
    extras = {
        "reward": float(result.state.metadata.get("tau_reward", 0.0)),
        "tau_info": result.state.metadata.get("tau_info", {}),
    }

    evaluators = [
        RuleBasedEvaluator(name="tau_reward_rule", min_reward=1.0),
        DSLEvaluator(name="tau_done_dsl", expression="extras['reward'] >= 1.0"),
    ]
    if include_model_judge:
        evaluators.append(ModelBasedEvaluator(name="tau_model_judge", llm=llm))

    suite = EvaluationSuite(evaluators=evaluators, mode="all")
    suite_res = suite.evaluate(EvaluationContext(task=task, run=result, manifest=manifest, extras=extras))
    return {
        "success": suite_res.success,
        "score": suite_res.score,
        "results": [
            {"name": r.name, "success": r.success, "score": r.score, "reasons": r.reasons}
            for r in suite_res.results
        ],
    }


def _run_one_task(args: argparse.Namespace, adapter: TauBenchAdapter, idx: int, record: Dict[str, Any], root: Path, trial: int = 0) -> Dict[str, Any]:
    started = time.time()
    task = adapter.to_task(record, split=args.tau_split, idx=idx)
    task.budget = TaskBudget(max_steps=int(args.max_steps))

    tau_env = _build_tau_env(args, task_index=idx)
    model = build_model_from_args(args)
    agent = TauBenchAgent(llm=model, tau_env=tau_env)

    trace_writer = make_trace_writer(args, f"tau_{args.tau_env}_{idx:05d}_trial{trial}")
    hooks = [] if args.disable_render else [ClaudeStyleHook(output_jsonl=str(root / "render_events.jsonl"), theme=args.theme)]

    engine_kwargs: Dict[str, Any] = {}
    if trace_writer is not None:
        engine_kwargs["trace_writer"] = trace_writer

    error_msg = None
    try:
        result = agent.run(
            task=task,
            return_state=True,
            hooks=hooks,
            max_steps=int(args.max_steps),
            task_index=idx,
            engine_kwargs=engine_kwargs,
        )
        eval_out = _evaluate_one(task=task, result=result, include_model_judge=bool(args.enable_model_judge), llm=model)
        reward = float(result.state.metadata.get("tau_reward", 0.0))
        stop_reason = result.state.stop_reason
        steps = result.step_count
    except Exception as exc:
        reward = 0.0
        stop_reason = "exception"
        steps = 0
        eval_out = {"success": False, "score": 0.0, "results": []}
        error_msg = str(exc)

    row = {
        "task_id": task.id,
        "idx": idx,
        "trial": trial,
        "benchmark": "tau-bench",
        "env": args.tau_env,
        "split": args.tau_split,
        "prediction": None,
        "reward": reward,
        "success": bool(eval_out.get("success", False)),
        "eval_score": float(eval_out.get("score", 0.0)),
        "eval_details": eval_out.get("results", []),
        "stop_reason": stop_reason,
        "steps": steps,
        "error": error_msg,
        "started_at": datetime.fromtimestamp(started, tz=timezone.utc).isoformat(),
        "ended_at": datetime.now(timezone.utc).isoformat(),
        "latency_seconds": round(time.time() - started, 3),
    }
    return row


def _print_metrics(rows: List[Dict[str, Any]]) -> None:
    metric_inputs = [
        MetricInput(
            task_id=str(r.get("task_id", "")),
            trial=int(r.get("trial", 0)),
            success=is_successful_reward(float(r.get("reward", 0.0))),
            reward=float(r.get("reward", 0.0)),
            steps=int(r.get("steps", 0)),
            latency_seconds=float(r.get("latency_seconds", 0.0)),
            stop_reason=str(r.get("stop_reason", "")),
            payload=r,
        )
        for r in rows
    ]
    registry = MetricRegistry(
        [
            RewardAverageMetric(),
            RewardSuccessRateMetric(),
            RewardPassHatMetric(),
            MeanStepsMetric(),
            StopReasonDistributionMetric(),
        ]
    )
    reports = {r.name: r for r in registry.compute_all(metric_inputs)}
    pass_hat_ks = reports["reward_pass_hat"].value if "reward_pass_hat" in reports else {}

    print("[Tau-Bench] Metrics (aligned with tau-bench run.py)")
    print(f"- avg_reward: {reports['avg_reward'].value if 'avg_reward' in reports else 0.0}")
    print("- pass^k:")
    for k in sorted(pass_hat_ks.keys()):
        print(f"  - k={k}: {pass_hat_ks[k]}")
    for rep in reports.values():
        print(f"- {rep.name}: {rep.value}")


def _run_full(args: argparse.Namespace, adapter: TauBenchAdapter, records: List[Dict[str, Any]], root: Path) -> None:
    start_idx = max(0, int(args.start_index))
    end_idx = len(records) if int(args.end_index) < 0 else min(len(records), int(args.end_index))
    indices = list(range(start_idx, end_idx))
    if int(args.limit) > 0:
        indices = indices[: int(args.limit)]

    all_jobs: List[tuple[int, int]] = []
    for trial in range(int(args.num_trials)):
        seq = list(indices)
        if bool(args.shuffle):
            rnd = random.Random(int(args.seed) + trial)
            rnd.shuffle(seq)
        for idx in seq:
            all_jobs.append((trial, idx))

    stamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
    out_path = Path(args.output_jsonl).expanduser().resolve() if args.output_jsonl else (root / f"tau_bench_{args.tau_env}_{args.tau_split}_{stamp}.jsonl")

    done = _read_done(out_path) if args.resume else set()
    jobs = [(t, i) for (t, i) in all_jobs if f"{t}:{i}" not in done]
    print(f"[Tau-Bench] env={args.tau_env} split={args.tau_split} total_jobs={len(all_jobs)} pending={len(jobs)}")

    rows: List[Dict[str, Any]] = []
    if int(args.concurrency) <= 1:
        for n, (trial, idx) in enumerate(jobs, start=1):
            row = _run_one_task(args=args, adapter=adapter, idx=idx, record=records[idx], root=root, trial=trial)
            _append_jsonl(out_path, {**row, "_job_key": f"{trial}:{idx}"})
            rows.append(row)
            print(f"[Tau-Bench] {n}/{len(jobs)} trial={trial} idx={idx} reward={row['reward']} success={row['success']}")
    else:
        with ThreadPoolExecutor(max_workers=int(args.concurrency)) as ex:
            futs = [
                ex.submit(_run_one_task, args=args, adapter=adapter, idx=idx, record=records[idx], root=root, trial=trial)
                for (trial, idx) in jobs
            ]
            for n, fut in enumerate(as_completed(futs), start=1):
                row = fut.result()
                _append_jsonl(out_path, {**row, "_job_key": f"{row['trial']}:{row['idx']}"})
                rows.append(row)
                print(
                    f"[Tau-Bench] {n}/{len(jobs)} trial={row['trial']} idx={row['idx']} "
                    f"reward={row['reward']} success={row['success']}"
                )

    print("output_jsonl:", out_path)
    _print_metrics(rows)


def main() -> None:
    ap = argparse.ArgumentParser()
    add_common_args(ap)
    ap.add_argument("--tau-env", default="retail", choices=["retail", "airline"])
    ap.add_argument("--tau-split", default="test", choices=["train", "dev", "test"])
    ap.add_argument("--task-index", type=int, default=0)
    ap.add_argument("--max-steps", type=int, default=30)

    ap.add_argument("--run-all", action="store_true")
    ap.add_argument("--start-index", type=int, default=0)
    ap.add_argument("--end-index", type=int, default=-1)
    ap.add_argument("--limit", type=int, default=0)
    ap.add_argument("--num-trials", type=int, default=1)
    ap.add_argument("--concurrency", type=int, default=1)
    ap.add_argument("--shuffle", action="store_true")
    ap.add_argument("--seed", type=int, default=10)
    ap.add_argument("--output-jsonl", default="")
    ap.add_argument("--resume", action="store_true")

    ap.add_argument("--user-strategy", default="llm")
    ap.add_argument("--user-model", default="gpt-4o")
    ap.add_argument("--user-model-provider", default="openai")
    ap.add_argument("--enable-model-judge", action="store_true")

    args = ap.parse_args()

    root, temp_ctx = setup_workspace(args.workspace)
    adapter = TauBenchAdapter(env_name=args.tau_env, task_split=args.tau_split)
    records = adapter.load_records(env_name=args.tau_env, split=args.tau_split)
    if not records:
        raise RuntimeError("No Tau-Bench tasks loaded.")

    if args.run_all:
        _run_full(args=args, adapter=adapter, records=records, root=root)
    else:
        idx = max(0, min(int(args.task_index), len(records) - 1))
        row = _run_one_task(args=args, adapter=adapter, idx=idx, record=records[idx], root=root, trial=0)
        print("workspace:", root)
        print("task_id:", row["task_id"])
        print("reward:", row["reward"])
        print("success:", row["success"])
        print("stop_reason:", row["stop_reason"])
        _print_metrics([row])

    if temp_ctx is not None:
        temp_ctx.cleanup()


if __name__ == "__main__":
    main()
